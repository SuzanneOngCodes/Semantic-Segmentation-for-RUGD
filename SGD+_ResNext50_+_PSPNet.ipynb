{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGD+ ResNext50 + PSPNet",
      "provenance": [],
      "authorship_tag": "ABX9TyMhO5WLYV2K2WJFIwH/VU3q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuzanneOngCodes/Semantic-segmentation/blob/main/SGD%2B_ResNext50_%2B_PSPNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ResNet50 + PSPNet backbone with the focus to test on performance betweeen different architectures**"
      ],
      "metadata": {
        "id": "H2RxLf-IRUQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eslhnttvQ7V5"
      },
      "outputs": [],
      "source": [
        "# Comment out %%capture to view downloading progress\n",
        "%%capture\n",
        "\n",
        "## If there is a problem in running the dataloader, try:\n",
        "# !pip uninstall albumentations\n",
        "## Restart runtime and continue\n",
        "!pip install --upgrade albumentations\n",
        "\n",
        "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
        "# !pip uninstall segmentation_models.pytorch\n",
        "!pip install segmentation-models-pytorch==0.2.0\n",
        "!pip install --upgrade transformers\n",
        "!pip install opencv-python-headless==4.5.2.52"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Initial setup - removing redundant directory\n",
        "## Average time to compute - approx. 1-2 mins\n",
        "# Comment out %%capture to view downloading progress\n",
        "%%capture\n",
        "!rm -rf sample_data/"
      ],
      "metadata": {
        "id": "C2BKZrUvRYCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment out %%capture to view downloading progress\n",
        "%%capture\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import shutil\n",
        "from torch import nn\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
        "import pandas as pd\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import albumentations as album\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "80kevouCRYCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset here. \n",
        "**In this project, we will be starting off with RUGD offroad dataset from** http://rugd.vision"
      ],
      "metadata": {
        "id": "1XpcgBioRkTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment out %%capture to view downloading progress\n",
        "%%capture\n",
        "\n",
        "if os.path.exists(\"/content/data\") == False : \n",
        "  !mkdir data\n",
        "  %cd data\n",
        "\n",
        "  # Download and unzip raw frames from videos\n",
        "  !wget http://rugd.vision/data/RUGD_frames-with-annotations.zip\n",
        "  !unzip RUGD_frames-with-annotations.zip\n",
        "\n",
        "  # Download and unzip raw annotations\n",
        "  !wget http://rugd.vision/data/RUGD_annotations.zip\n",
        "  !unzip RUGD_annotations.zip\n",
        "\n",
        "  # Remove zip files\n",
        "  !rm -r RUGD_annotations.zip\n",
        "  !rm -r RUGD_frames-with-annotations.zip\n",
        "\n",
        "  %mkdir \"RUGD_frames-with-annotations\"/training \n",
        "  %mkdir \"RUGD_frames-with-annotations\"/validation\n",
        "  %mkdir \"RUGD_frames-with-annotations\"/testing\n",
        "  %mkdir RUGD_annotations/training \n",
        "  %mkdir RUGD_annotations/validation\n",
        "  %mkdir RUGD_annotations/testing"
      ],
      "metadata": {
        "id": "dRn29yF7RYEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define labels based on classes"
      ],
      "metadata": {
        "id": "K7PWqee4Ro1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    0: 'dirt',\n",
        "    1: 'sand',\n",
        "    2: 'grass',\n",
        "    3: 'tree',\n",
        "    4: 'pole',\n",
        "    5: 'water',\n",
        "    6: 'sky',\n",
        "    7: 'vehicle',\n",
        "    8: 'container/generic-object',\n",
        "    9: 'asphalt',\n",
        "    10: 'gravel',\n",
        "    11: 'building',\n",
        "    12: 'mulch',\n",
        "    13: 'rock-bed',\n",
        "    14: 'log',\n",
        "    15: 'bicycle',\n",
        "    16: 'person',\n",
        "    17: 'fence',\n",
        "    18: 'bush',\n",
        "    19: 'sign',\n",
        "    20: 'rock',\n",
        "    21: 'bridge',\n",
        "    22: 'concrete', \n",
        "    23: 'picnic-table'\n",
        " }\n",
        "\n",
        "id2label = {int(k): v for k, v in id2label.items()}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "## Check if there are 24 labels\n",
        "num_labels = len(id2label)\n",
        "print(num_labels)\n",
        "CLASSES = [\"void\",\"dirt\", \"sand\", \"grass\", \"tree\", \"pole\", \"water\", \"sky\", \n",
        "        \"vehicle\", \"container/generic-object\", \"asphalt\", \"gravel\", \n",
        "        \"building\", \"mulch\", \"rock-bed\", \"log\", \"bicycle\", \"person\", \n",
        "        \"fence\", \"bush\", \"sign\", \"rock\", \"bridge\", \"concrete\", \"picnic-table\"]\n",
        "\n",
        "COLORMAP = [ [0,0,0],[ 108, 64, 20 ], [ 255, 229, 204 ],[ 0, 102, 0 ],[ 0, 255, 0 ],\n",
        "            [ 0, 153, 153 ],[ 0, 128, 255 ],[ 0, 0, 255 ],[ 255, 255, 0 ],[ 255, 0, 127 ],\n",
        "            [ 64, 64, 64 ],[ 255, 128, 0 ],[ 255, 0, 0 ],[ 153, 76, 0 ],[ 102, 102, 0 ],\n",
        "            [ 102, 0, 0 ],[ 0, 255, 128 ],[ 204, 153, 255 ],[ 102, 0, 204 ],[ 255, 153, 204 ],\n",
        "            [ 0, 102, 102 ],[ 153, 204, 255 ],[ 102, 255, 255 ],[ 101, 101, 11 ],[ 114, 85, 47 ] ]\n",
        "\n",
        "color_id = {tuple(c):i for i, c in enumerate(COLORMAP)}\n",
        "\n",
        "select_class_indices = [CLASSES.index(cls.lower()) for cls in CLASSES]\n",
        "select_class_rgb_values =  np.array(COLORMAP)[select_class_indices]"
      ],
      "metadata": {
        "id": "FRvKTIbvRYGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "KHuUfxksRubA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to visualize on a sample image \n",
        "def visualize(**images):\n",
        "    \"\"\"\n",
        "    Plot images in one row\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for idx, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n_images, idx + 1)\n",
        "        plt.xticks([]); \n",
        "        plt.yticks([])\n",
        "        # get title from the parameter names\n",
        "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "# Perform one hot encoding on label\n",
        "def one_hot_encode(label, label_values):\n",
        "    \"\"\"\n",
        "    Convert a segmentation image label array to one-hot format\n",
        "    by replacing each pixel value with a vector of length num_classes\n",
        "    # Arguments\n",
        "        label: The 2D array segmentation image label\n",
        "        label_values\n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but\n",
        "        with a depth size of num_classes\n",
        "    \"\"\"\n",
        "    semantic_map = []\n",
        "    for colour in label_values:\n",
        "        equality = np.equal(label, colour)\n",
        "        class_map = np.all(equality, axis = -1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1)\n",
        "\n",
        "    return semantic_map\n",
        "    \n",
        "# Perform reverse one-hot-encoding on labels / preds\n",
        "def reverse_one_hot(image):\n",
        "    \"\"\"\n",
        "    Transform a 2D array in one-hot format (depth is num_classes),\n",
        "    to a 2D array with only 1 channel, where each pixel value is\n",
        "    the classified class key.\n",
        "    # Arguments\n",
        "        image: The one-hot format image \n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but\n",
        "        with a depth size of 1, where each pixel value is the classified \n",
        "        class key.\n",
        "    \"\"\"\n",
        "    x = np.argmax(image, axis = -1)\n",
        "    return x\n",
        "\n",
        "# Perform colour coding on the reverse-one-hot outputs\n",
        "def colour_code_segmentation(image, label_values):\n",
        "    \"\"\"\n",
        "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
        "    # Arguments\n",
        "        image: single channel array where each value represents the class key.\n",
        "        label_values\n",
        "\n",
        "    # Returns\n",
        "        Colour coded image for segmentation visualization\n",
        "    \"\"\"\n",
        "    colour_codes = np.array(label_values)\n",
        "    x = colour_codes[image.astype(int)]\n",
        "\n",
        "    return x\n",
        "\n",
        "# classes for data loading and preprocessing\n",
        "class SemanticSegmentationDataset():\n",
        "\n",
        "    def __init__(self, root_dir, train, colormap = select_class_rgb_values, augmentation = None, preprocessing=None):\n",
        "        \n",
        "        self.root_dir = root_dir\n",
        "        self.train = train\n",
        "        self.colormap = colormap\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "        self.transform = torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        self.img_dir = os.path.join(self.root_dir, \"RUGD_frames-with-annotations\")\n",
        "        self.ann_dir = os.path.join(self.root_dir, \"RUGD_annotations\")\n",
        "        \n",
        "        # read images and allocate them to training, validation and testing sets \n",
        "        sub_train = [\"creek\", \"park-1\", \"park-2\", \"trail-4\", \"trail-11\",\"trail-12\",\"trail-13\",\"trail-14\", \"trail-5\", \"trail-6\"]\n",
        "        sub_valid = [\"trail-7\",\"trail-9\", \"village\"]\n",
        "        sub_test = [\"trail-15\", \"trail-3\", \"park-8\",\"trail\",\"trail-10\"]\n",
        "\n",
        "        places = []\n",
        "        subpath = \"\"\n",
        "        if self.train==1:\n",
        "          places = sub_train\n",
        "          subpath = \"training\"\n",
        "        elif self.train == 2:\n",
        "          places = sub_test\n",
        "          subpath = \"testing\"\n",
        "        else:\n",
        "          places = sub_valid\n",
        "          subpath = \"validation\"\n",
        "\n",
        "        image_file_names = [] \n",
        "        annotation_file_names = []\n",
        "        self.img_directory = os.path.join(self.img_dir, subpath)\n",
        "        self.ann_directory = os.path.join(self.ann_dir, subpath)\n",
        "\n",
        "        for i in places:\n",
        "          source_dir = os.path.join(self.img_dir, i)\n",
        "          source_a_dir = os.path.join(self.ann_dir, i)\n",
        "\n",
        "          for j in os.listdir(source_dir):\n",
        "            shutil.copy(os.path.join(source_dir,j), self.img_directory) ## Or shutil.set to save memory\n",
        "\n",
        "          for j in os.listdir(source_a_dir):\n",
        "            shutil.copy(os.path.join(source_a_dir,j), self.ann_directory)\n",
        "          \n",
        "\n",
        "        ## Make sure that all frames matches with the assigned annotations\n",
        "        for root, dirs, files in os.walk(self.img_directory):\n",
        "          image_file_names.extend(files)\n",
        "          annotation_file_names.extend(files)\n",
        "          \n",
        "        self.images = sorted(image_file_names)\n",
        "        self.annotations = sorted(annotation_file_names)\n",
        "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv.cvtColor(cv.imread(os.path.join(self.img_directory,self.images[idx])), cv.COLOR_BGR2RGB)\n",
        "        mask = cv.cvtColor(cv.imread(os.path.join(self.ann_directory,self.annotations[idx])), cv.COLOR_BGR2RGB)\n",
        "        \n",
        "        # one-hot-encode the mask\n",
        "        mask = one_hot_encode(mask, self.colormap).astype('float')\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            \n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "l2pPwBDVRYJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize width and height\n",
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "        album.Resize(304, 304),\n",
        "        album.PadIfNeeded(min_height=100, min_width=100, always_apply=True, border_mode=0),\n",
        "        album.OneOf(\n",
        "            [\n",
        "                album.HorizontalFlip(p=1),\n",
        "                album.VerticalFlip(p=1),\n",
        "                album.RandomRotate90(p=1),\n",
        "            ],\n",
        "            p=0.5,\n",
        "        ),\n",
        "    ]\n",
        "    return album.Compose(train_transform)\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    train_transform = [\n",
        "        album.Resize(304, 304),\n",
        "        album.PadIfNeeded(min_height=100, min_width=100, always_apply=True, border_mode=0),\n",
        "    ]\n",
        "    return album.Compose(train_transform)\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn=None):\n",
        "    \"\"\"Construct preprocessing transform    \n",
        "    Args:\n",
        "        preprocessing_fn (callable): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \"\"\"   \n",
        "    _transform = []\n",
        "    if preprocessing_fn:\n",
        "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
        "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
        "        \n",
        "    return album.Compose(_transform)\n"
      ],
      "metadata": {
        "id": "eRsm_ewJRxQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the dataset"
      ],
      "metadata": {
        "id": "pNPDfEuGR16w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all information on the training, testing and validation sets\n",
        "train_set = SemanticSegmentationDataset(root_dir=\"/content/data\", train = 1, augmentation = get_training_augmentation())\n",
        "test_set = SemanticSegmentationDataset(root_dir=\"/content/data\", train = 2)\n",
        "val_set = SemanticSegmentationDataset(root_dir=\"/content/data\", train = 3)"
      ],
      "metadata": {
        "id": "3sXWWp5PRxRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Length of all sets\n",
        "print(\"Number of training examples:\", len(train_set))\n",
        "print(\"Number of validation examples:\", len(val_set))\n",
        "print(\"Number of testing examples:\", len(test_set))"
      ],
      "metadata": {
        "id": "jvkNbcEbRxTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check format\n",
        "random_idx = np.random.randint(0, len(train_set)-1)\n",
        "image, mask = train_set[random_idx]\n",
        "\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
        ")\n",
        "reverse_one_hot(mask).shape"
      ],
      "metadata": {
        "id": "-QPysVKvRxWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establish encoder"
      ],
      "metadata": {
        "id": "3tfJ5hoNR9_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "ENCODER = 'se_resnext50_32x4d'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        "\n",
        "# create segmentation model with pretrained encoder\n",
        "model = smp.PSPNet(\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=len(CLASSES), \n",
        "    activation=ACTIVATION,\n",
        ")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ],
      "metadata": {
        "id": "u7AIBec9RxYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estabish decoder and optimizer"
      ],
      "metadata": {
        "id": "DLQUIskHSDiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
        "# !pip uninstall segmentation_models.pytorch\n",
        "# !pip install segmentation-models-pytorch==0.2.0\n",
        "\n",
        "TRAINING = True\n",
        "\n",
        "# Set num of epochs\n",
        "EPOCHS = 1\n",
        "\n",
        "# Set device: `cuda` or `cpu`\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# define loss function\n",
        "loss = smp.utils.losses.DiceLoss()\n",
        "\n",
        "# define metrics\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "    smp.utils.metrics.Fscore(threshold=0.5),\n",
        "    smp.utils.metrics.Accuracy(threshold=0.5),\n",
        "    smp.utils.metrics.Recall(threshold=0.5),\n",
        "    smp.utils.metrics.Precision(threshold=0.5),\n",
        "]\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.SGD([ \n",
        "    dict(params=model.parameters(), lr=0.0001, momentum =0.9),\n",
        "])\n",
        "\n",
        "# define learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
        "    optimizer, gamma=0.01,\n",
        ")\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "# load best saved model checkpoint from previous commit (if present)\n",
        "if os.path.exists('/content/drive/MyDrive/Colab Notebooks/best_modelPSPSGD.pth'):\n",
        "    model = torch.load('/content/drive/MyDrive/Colab Notebooks/best_modelPSPSGD.pth', map_location=DEVICE)"
      ],
      "metadata": {
        "id": "Wc1EydEcRxa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get epochs and augmented datasets for training and validation sets"
      ],
      "metadata": {
        "id": "lV53-LLPSJug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "9PK5mgv-Rxdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations\n",
        "# Get train and val dataset instances\n",
        "train_dataset = SemanticSegmentationDataset(\n",
        "    root_dir='/content/data', \n",
        "    augmentation=get_training_augmentation(),\n",
        "    train = 1,\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        ")\n",
        "\n",
        "valid_dataset = SemanticSegmentationDataset(\n",
        "    root_dir = '/content/data', \n",
        "    augmentation=get_training_augmentation(),\n",
        "    train=3,\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        ")"
      ],
      "metadata": {
        "id": "b16cQUMlRxfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloaders"
      ],
      "metadata": {
        "id": "IWB2fxFtSQpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get train and val data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=20, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "RaN0CMTLRxiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the model**\n",
        "The optimal so far is 3 epochs, but feel free to adjust the number at EPOCHS in line 12"
      ],
      "metadata": {
        "id": "Ns2aN_k7SVq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if TRAINING:\n",
        "\n",
        "    best_iou_score = 0.0\n",
        "    train_logs_list, valid_logs_list = [], []\n",
        "\n",
        "    for i in range(0, EPOCHS):\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Perform training & validation\n",
        "        print('\\nEpoch: {}'.format(i))\n",
        "        train_logs = train_epoch.run(train_loader)\n",
        "        valid_logs = valid_epoch.run(valid_loader)\n",
        "        train_logs_list.append(train_logs)\n",
        "        valid_logs_list.append(valid_logs)\n",
        "\n",
        "        # Save model if a better val IoU score is obtained\n",
        "        if best_iou_score < valid_logs['iou_score']:\n",
        "            best_iou_score = valid_logs['iou_score']\n",
        "            torch.save(model, './best_modelPSPSGD.pth')\n",
        "            print('Model saved!')\n",
        "\n",
        "        lr_scheduler.step()"
      ],
      "metadata": {
        "id": "vW-tnMS0Rxke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load best saved model checkpoint from the current run\n",
        "if os.path.exists('/content/data/best_modelPSPSGD.pth'):\n",
        "    model = torch.load('/content/data/best_modelPSPSGD.pth', map_location=DEVICE)\n",
        "    print('Loaded PSPNet model from this run.')\n",
        "model"
      ],
      "metadata": {
        "id": "U5D79EReSZAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run model on **testing set**"
      ],
      "metadata": {
        "id": "Kz3ndtjASfcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create test dataloader (with preprocessing operation: to_tensor(...))\n",
        "test_dataset = SemanticSegmentationDataset(\n",
        "    root_dir = \"/content/data\", \n",
        "    augmentation=get_validation_augmentation(), \n",
        "    train = 2,\n",
        ")\n",
        "\n",
        "# test dataset for visualization (without preprocessing augmentations & transformations)\n",
        "# get a random test image/mask index\n",
        "random_idx = np.random.randint(0, len(test_dataset)-1)\n",
        "image, mask = test_dataset[random_idx]\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
        ")"
      ],
      "metadata": {
        "id": "PH80YBpESdJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_folder = '/content/predictionsPSPSGD/'\n",
        "if not os.path.exists(preds_folder):\n",
        "    os.makedirs(\"/content/predictionsPSPSGD/\")"
      ],
      "metadata": {
        "id": "leAjQQZ1Sh_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_epoch = smp.utils.train.ValidEpoch(\n",
        "    model,\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "test_dataset = SemanticSegmentationDataset(\n",
        "    root_dir='/content/data', \n",
        "    augmentation=get_validation_augmentation(),\n",
        "    train = 2,\n",
        "    preprocessing=get_preprocessing(preprocessing_fn),\n",
        ")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=True, num_workers=0)\n",
        "test_logs = test_epoch.run(test_dataloader)\n",
        "\n",
        "test_logs"
      ],
      "metadata": {
        "id": "RZEmlKZGSkF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference, Metrics Evaluation and Visualisation**"
      ],
      "metadata": {
        "id": "Yi_tQKU8SnKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation on Test Data: \")\n",
        "print(f\"Mean Accuracy: {test_logs['accuracy']:.4f}\")\n",
        "print(f\"Mean Dice Loss: {test_logs['dice_loss']:.4f}\")\n",
        "print(f\"Mean F score: {test_logs['fscore']:.4f}\")\n",
        "print(f\"Mean IoU Score: {test_logs['iou_score']:.4f}\")\n",
        "print(f\"Mean Precision: {test_logs['precision']:.4f}\")\n",
        "print(f\"Mean Recall: {test_logs['recall']:.4f}\")"
      ],
      "metadata": {
        "id": "Saed0RjuSn4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## For sample illustrations\n",
        "# Center crop padded image / mask to original image dims\n",
        "def crop_image(image, true_dimensions):\n",
        "    return album.CenterCrop(p=1,height=304, width=304)(image=image)\n",
        "\n",
        "# test dataset for visualization\n",
        "test_dataset_vis = SemanticSegmentationDataset(\n",
        "    root_dir='/content/data',\n",
        "    augmentation = get_validation_augmentation(),\n",
        "    train = 2,\n",
        ")"
      ],
      "metadata": {
        "id": "G01IMUuhSpyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(500):\n",
        "    image, gt_mask = test_dataset[idx]\n",
        "    image_vis = test_dataset_vis[idx][0].astype('uint8')\n",
        "    true_dimensions = image_vis.shape\n",
        "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    # Predict test image\n",
        "    pred_mask = model(x_tensor)\n",
        "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
        "    # Convert pred_mask from `CHW` format to `HWC` format\n",
        "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
        "    # Get prediction channel corresponding to sky, or any classes available\n",
        "    pred_foreground_heatmap = crop_image(pred_mask[:,:,CLASSES.index(\"sky\")], true_dimensions)['image']\n",
        "    pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values), true_dimensions)['image']\n",
        "    # Convert gt_mask from `CHW` format to `HWC` format\n",
        "    gt_mask = np.transpose(gt_mask,(1,2,0))\n",
        "    gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values), true_dimensions)['image']\n",
        "    # cv.imwrite(os.path.join(preds_folder, f\"pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n",
        "    cv.imwrite(os.path.join(preds_folder, f\"pred_{idx}.png\"), np.hstack([pred_mask])[::-1])\n",
        "\n",
        "    visualize(\n",
        "        original_image = image_vis,\n",
        "        ground_truth_mask = gt_mask,\n",
        "        predicted_mask = pred_mask,\n",
        "        pred_foreground_heatmap = pred_foreground_heatmap\n",
        "    )"
      ],
      "metadata": {
        "id": "Evzqcyn7Ssre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Copy all info:\n",
        "with open('inferencePSPSGD.txt', 'w') as convert_file:\n",
        "  convert_file.write(\"Training on each epoch\")\n",
        "  convert_file.write(str(train_logs_list))\n",
        "  convert_file.write(\"\\nTraining means\")\n",
        "  convert_file.write(str(train_logs))\n",
        "  convert_file.write(\"\\nValid on each epoch\")\n",
        "  convert_file.write(str(valid_logs_list))\n",
        "  convert_file.write(\"\\nValid means\")\n",
        "  convert_file.write(str(valid_logs))\n",
        "  convert_file.write(\"\\nTested means\")\n",
        "  convert_file.write(str(test_logs))"
      ],
      "metadata": {
        "id": "S9zq2CRYSwac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Download predictions and trained model\n",
        "!zip -r /content/predictionsPSPSGD.zip /content/predictionsPSPSGD\n",
        "from google.colab import files\n",
        "files.download(\"/content/predictionsPSPSGD.zip\")\n",
        "files.download(\"/content/data/best_modelPSPSGD.pth\")\n",
        "files.download(\"/content/data/inferencePSPSGD.txt\")"
      ],
      "metadata": {
        "id": "nuPsjVqLSxD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}