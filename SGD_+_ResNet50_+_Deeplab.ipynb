{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGD + ResNet50 + Deeplab",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsujtrQFtU0dkMYGnUvrTF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuzanneOngCodes/Semantic-segmentation/blob/main/SGD_%2B_ResNet50_%2B_Deeplab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ResNet50 + Deeplab + SGD backbone with the focus to test on performance betweeen different architectures**"
      ],
      "metadata": {
        "id": "IkFWLGppOhxo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHNlZHsxODsi"
      },
      "outputs": [],
      "source": [
        "# Comment out %%capture to view downloading progress\n",
        "%%capture\n",
        " \n",
        "## If there is a problem in running the dataloader, try:\n",
        "# !pip uninstall albumentations\n",
        "## Restart runtime and continue\n",
        "!pip install --upgrade albumentations\n",
        "\n",
        "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
        "# !pip uninstall segmentation_models.pytorch\n",
        "!pip install segmentation-models-pytorch==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Initial setup - removing redundant directory\n",
        "## Average time to compute - approx. 1-2 mins\n",
        "# Comment out %%capture to view downloading progress\n",
        "%%capture\n",
        "!rm -rf sample_data/\n",
        "\n",
        "!pip install opencv-python-headless==4.5.2.52\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "qV0ahfaoOnUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment out %%capture to view downloading progress\n",
        "%%capture\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import shutil\n",
        "from torch import nn\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import albumentations as album\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "lP1eyWMSOpnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset here. \n",
        "**In this project, we will be starting off with RUGD offroad dataset from** http://rugd.vision"
      ],
      "metadata": {
        "id": "7cZB-nZkOwZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment out %%capture to view downloading progress\n",
        "%%capture\n",
        "\n",
        "if os.path.exists(\"/content/data\") == False : \n",
        "  !mkdir data\n",
        "  %cd data\n",
        "\n",
        "  # Download and unzip raw frames from videos\n",
        "  !wget http://rugd.vision/data/RUGD_frames-with-annotations.zip\n",
        "  !unzip RUGD_frames-with-annotations.zip\n",
        "\n",
        "  # Download and unzip raw annotations\n",
        "  !wget http://rugd.vision/data/RUGD_annotations.zip\n",
        "  !unzip RUGD_annotations.zip\n",
        "\n",
        "  # Remove zip files\n",
        "  !rm -r RUGD_annotations.zip\n",
        "  !rm -r RUGD_frames-with-annotations.zip\n",
        "\n",
        "  %mkdir \"RUGD_frames-with-annotations\"/training \n",
        "  %mkdir \"RUGD_frames-with-annotations\"/validation\n",
        "  %mkdir \"RUGD_frames-with-annotations\"/testing\n",
        "  %mkdir RUGD_annotations/training \n",
        "  %mkdir RUGD_annotations/validation\n",
        "  %mkdir RUGD_annotations/testing"
      ],
      "metadata": {
        "id": "dIJHkSmZOujH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define labels based on classes"
      ],
      "metadata": {
        "id": "8T2O9bXEO4ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        "    0: 'dirt',\n",
        "    1: 'sand',\n",
        "    2: 'grass',\n",
        "    3: 'tree',\n",
        "    4: 'pole',\n",
        "    5: 'water',\n",
        "    6: 'sky',\n",
        "    7: 'vehicle',\n",
        "    8: 'container/generic-object',\n",
        "    9: 'asphalt',\n",
        "    10: 'gravel',\n",
        "    11: 'building',\n",
        "    12: 'mulch',\n",
        "    13: 'rock-bed',\n",
        "    14: 'log',\n",
        "    15: 'bicycle',\n",
        "    16: 'person',\n",
        "    17: 'fence',\n",
        "    18: 'bush',\n",
        "    19: 'sign',\n",
        "    20: 'rock',\n",
        "    21: 'bridge',\n",
        "    22: 'concrete', \n",
        "    23: 'picnic-table'\n",
        " }\n",
        "\n",
        "id2label = {int(k): v for k, v in id2label.items()}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "## Check if there are 24 labels\n",
        "num_labels = len(id2label)\n",
        "print(num_labels)\n",
        "CLASSES = [\"dirt\", \"sand\", \"grass\", \"tree\", \"pole\", \"water\", \"sky\", \n",
        "        \"vehicle\", \"container/generic-object\", \"asphalt\", \"gravel\", \n",
        "        \"building\", \"mulch\", \"rock-bed\", \"log\", \"bicycle\", \"person\", \n",
        "        \"fence\", \"bush\", \"sign\", \"rock\", \"bridge\", \"concrete\", \"picnic-table\"]\n",
        "\n",
        "COLORMAP = [[ 108, 64, 20 ], [ 255, 229, 204 ],[ 0, 102, 0 ],[ 0, 255, 0 ],\n",
        "            [ 0, 153, 153 ],[ 0, 128, 255 ],[ 0, 0, 255 ],[ 255, 255, 0 ],[ 255, 0, 127 ],\n",
        "            [ 64, 64, 64 ],[ 255, 128, 0 ],[ 255, 0, 0 ],[ 153, 76, 0 ],[ 102, 102, 0 ],\n",
        "            [ 102, 0, 0 ],[ 0, 255, 128 ],[ 204, 153, 255 ],[ 102, 0, 204 ],[ 255, 153, 204 ],\n",
        "            [ 0, 102, 102 ],[ 153, 204, 255 ],[ 102, 255, 255 ],[ 101, 101, 11 ],[ 114, 85, 47 ] ]\n",
        "\n",
        "color_id = {tuple(c):i for i, c in enumerate(COLORMAP)}\n",
        "\n",
        "select_class_indices = [CLASSES.index(cls.lower()) for cls in CLASSES]\n",
        "select_class_rgb_values =  np.array(COLORMAP)[select_class_indices]"
      ],
      "metadata": {
        "id": "wISl76qOO2tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "XnX4KovvPDjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize width and height\n",
        "def get_augmentation():\n",
        "    train_transform = [\n",
        "        album.Resize(304, 304),\n",
        "        album.PadIfNeeded(min_height=100, min_width=100, always_apply=True, border_mode=0),\n",
        "        album.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        album.OneOf(\n",
        "            [\n",
        "                album.HorizontalFlip(p=1),\n",
        "                album.VerticalFlip(p=1),\n",
        "                album.RandomRotate90(p=1),\n",
        "            ],\n",
        "            p=0.5,\n",
        "        ),\n",
        "    ]\n",
        "    return album.Compose(train_transform)\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    train_transform = [\n",
        "        album.Resize(304, 304),\n",
        "        album.PadIfNeeded(min_height=100, min_width=100, always_apply=True, border_mode=0),\n",
        "    ]\n",
        "    return album.Compose(train_transform)\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "def get_preprocessing(preprocessing_fn=None):\n",
        "    _transform = []\n",
        "    if preprocessing_fn:\n",
        "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
        "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
        "        \n",
        "    return album.Compose(_transform)"
      ],
      "metadata": {
        "id": "4ZBlpNpBPB7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to visualize on a sample image \n",
        "def visualize(**images):\n",
        "    \"\"\"\n",
        "    Plot images in one row\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for idx, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n_images, idx + 1)\n",
        "        plt.xticks([]); \n",
        "        plt.yticks([])\n",
        "        # get title from the parameter names\n",
        "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "# Perform one hot encoding on label\n",
        "def one_hot_encode(label, label_values):\n",
        "    \"\"\"\n",
        "    Convert a segmentation image label array to one-hot format\n",
        "    by replacing each pixel value with a vector of length num_classes\n",
        "    # Arguments\n",
        "        label: The 2D array segmentation image label\n",
        "        label_values\n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but\n",
        "        with a depth size of num_classes\n",
        "    \"\"\"\n",
        "    semantic_map = []\n",
        "    for colour in label_values:\n",
        "        equality = np.equal(label, colour)\n",
        "        class_map = np.all(equality, axis = -1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1)\n",
        "\n",
        "    return semantic_map\n",
        "    \n",
        "# Perform reverse one-hot-encoding on labels / preds\n",
        "def reverse_one_hot(image):\n",
        "    \"\"\"\n",
        "    Transform a 2D array in one-hot format (depth is num_classes),\n",
        "    to a 2D array with only 1 channel, where each pixel value is\n",
        "    the classified class key.\n",
        "    # Arguments\n",
        "        image: The one-hot format image \n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but\n",
        "        with a depth size of 1, where each pixel value is the classified \n",
        "        class key.\n",
        "    \"\"\"\n",
        "    x = np.argmax(image, axis = -1)\n",
        "    return x\n",
        "\n",
        "# Perform colour coding on the reverse-one-hot outputs\n",
        "def colour_code_segmentation(image, label_values):\n",
        "    \"\"\"\n",
        "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
        "    # Arguments\n",
        "        image: single channel array where each value represents the class key.\n",
        "        label_values\n",
        "\n",
        "    # Returns\n",
        "        Colour coded image for segmentation visualization\n",
        "    \"\"\"\n",
        "    colour_codes = np.array(label_values)\n",
        "    x = colour_codes[image.astype(int)]\n",
        "\n",
        "    return x\n",
        "\n",
        "# classes for data loading and preprocessing\n",
        "class SemanticSegmentationDataset():\n",
        "\n",
        "    def __init__(self, root_dir, train, colormap = select_class_rgb_values, augmentation = None, preprocessing=None):\n",
        "        \n",
        "        self.root_dir = root_dir\n",
        "        self.train = train\n",
        "        self.colormap = colormap\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "        self.transform = torchvision.transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        self.img_dir = os.path.join(self.root_dir, \"RUGD_frames-with-annotations\")\n",
        "        self.ann_dir = os.path.join(self.root_dir, \"RUGD_annotations\")\n",
        "        \n",
        "        # read images and allocate them to training, validation and testing sets \n",
        "        sub_train = [\"creek\", \"park-1\", \"park-2\", \"trail-4\", \"trail-11\",\"trail-12\",\"trail-13\",\"trail-14\", \"trail-5\", \"trail-6\"]\n",
        "        sub_valid = [\"trail-7\",\"trail-9\", \"village\"]\n",
        "        sub_test = [\"trail-15\", \"trail-3\", \"park-8\",\"trail\",\"trail-10\"]\n",
        "\n",
        "        places = []\n",
        "        subpath = \"\"\n",
        "        if self.train==1:\n",
        "          places = sub_train\n",
        "          subpath = \"training\"\n",
        "        elif self.train == 2:\n",
        "          places = sub_test\n",
        "          subpath = \"testing\"\n",
        "        else:\n",
        "          places = sub_valid\n",
        "          subpath = \"validation\"\n",
        "\n",
        "        image_file_names = [] \n",
        "        annotation_file_names = []\n",
        "        self.img_directory = os.path.join(self.img_dir, subpath)\n",
        "        self.ann_directory = os.path.join(self.ann_dir, subpath)\n",
        "\n",
        "        for i in places:\n",
        "          source_dir = os.path.join(self.img_dir, i)\n",
        "          source_a_dir = os.path.join(self.ann_dir, i)\n",
        "\n",
        "          for j in os.listdir(source_dir):\n",
        "            shutil.copy(os.path.join(source_dir,j), self.img_directory) ## Or shutil.set to save memory\n",
        "\n",
        "          for j in os.listdir(source_a_dir):\n",
        "            shutil.copy(os.path.join(source_a_dir,j), self.ann_directory)\n",
        "          \n",
        "\n",
        "        ## Make sure that all frames matches with the assigned annotations\n",
        "        for root, dirs, files in os.walk(self.img_directory):\n",
        "          image_file_names.extend(files)\n",
        "          annotation_file_names.extend(files)\n",
        "          \n",
        "        self.images = sorted(image_file_names)\n",
        "        self.annotations = sorted(annotation_file_names)\n",
        "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv.cvtColor(cv.imread(os.path.join(self.img_directory,self.images[idx])), cv.COLOR_BGR2RGB)\n",
        "        mask = cv.cvtColor(cv.imread(os.path.join(self.ann_directory,self.annotations[idx])), cv.COLOR_BGR2RGB)\n",
        "        \n",
        "        # one-hot-encode the mask\n",
        "        mask = one_hot_encode(mask, self.colormap).astype('float')\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            \n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "3sHLnkqYPLAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the dataset"
      ],
      "metadata": {
        "id": "ruu_URTzPOE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Get all information on the training, testing and validation sets\n",
        "# train_set = SemanticSegmentationDataset(root_dir=\"/content/data\", train = 1)\n",
        "# test_set = SemanticSegmentationDataset(root_dir=\"/content/data\", train = 2)\n",
        "val_set = SemanticSegmentationDataset(root_dir=\"/content/data\", train = 3)"
      ],
      "metadata": {
        "id": "iOQdpue1PNIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Length of all sets\n",
        "#print(\"Number of training examples:\", len(train_set))\n",
        "print(\"Number of validation examples:\", len(val_set))\n",
        "# print(\"Number of testing examples:\", len(test_set))"
      ],
      "metadata": {
        "id": "pA0NYxw7PTs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check format\n",
        "random_idx = np.random.randint(0, len(val_set)-1)\n",
        "image, mask = val_set[random_idx]\n",
        "\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
        ")\n",
        "reverse_one_hot(mask).shape"
      ],
      "metadata": {
        "id": "13FHunxdPVmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establish architecture"
      ],
      "metadata": {
        "id": "nPSJj8VMPYiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "ENCODER = 'resnet50'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "ACTIVATION = 'sigmoid' \n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ],
      "metadata": {
        "id": "6ro9yTxzPXuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "deeplab = models.segmentation.deeplabv3_resnet50(pretrained=0, progress=1, num_classes=len(CLASSES))\n",
        "\n",
        "class SegModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SegModel,self).__init__()\n",
        "        self.dl = deeplab\n",
        "        \n",
        "    def forward(self, x):\n",
        "        y = self.dl(x)['out']\n",
        "        return y"
      ],
      "metadata": {
        "id": "0kZiMVscPd8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Estabish model and optimizer"
      ],
      "metadata": {
        "id": "ACNNorU7PfwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Set device: `cuda` or `cpu`\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SegModel()\n",
        "\n",
        "loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.SGD([ \n",
        "    dict(params=model.parameters(), lr=0.0001, momentum = 0.9),\n",
        "])\n",
        "\n",
        "# define learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
        "    optimizer, gamma=0.001,\n",
        ")\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "# load best saved model checkpoint from previous commit (if present)\n",
        "if os.path.exists('/content/drive/MyDrive/Colab Notebooks/best_modelDeeplabSGD3.pth'):\n",
        "    model = torch.load('/content/drive/MyDrive/Colab Notebooks/best_modelDeeplabSGD3.pth', map_location=DEVICE)\n"
      ],
      "metadata": {
        "id": "vS25_m1VPetY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def meanIOU(target, predicted):\n",
        "    if target.shape != predicted.shape:\n",
        "        print(\"target has dimension\", target.shape, \", predicted values have shape\", predicted.shape)\n",
        "        return\n",
        "        \n",
        "    if target.dim() != 4:\n",
        "        print(\"target has dim\", target.dim(), \", Must be 4.\")\n",
        "        return\n",
        "    \n",
        "    iousum = 0\n",
        "    for i in range(target.shape[0]):\n",
        "        target_arr = target[i, :, :, :].clone().detach().cpu().numpy().argmax(0)\n",
        "        predicted_arr = predicted[i, :, :, :].clone().detach().cpu().numpy().argmax(0)\n",
        "        \n",
        "        intersection = np.logical_and(target_arr, predicted_arr).sum()\n",
        "        union = np.logical_or(target_arr, predicted_arr).sum()\n",
        "        if union == 0:\n",
        "            iou_score = 0\n",
        "        else :\n",
        "            iou_score = intersection / union\n",
        "        iousum +=iou_score\n",
        "        \n",
        "    miou = iousum/target.shape[0]\n",
        "    return miou\n",
        "\n",
        "def pixelAcc(target, predicted):    \n",
        "    if target.shape != predicted.shape:\n",
        "        print(\"target has dimension\", target.shape, \", predicted values have shape\", predicted.shape)\n",
        "        return\n",
        "        \n",
        "    if target.dim() != 4:\n",
        "        print(\"target has dim\", target.dim(), \", Must be 4.\")\n",
        "        return\n",
        "    \n",
        "    accsum=0\n",
        "    for i in range(target.shape[0]):\n",
        "        target_arr = target[i, :, :, :].clone().detach().cpu().numpy().argmax(0)\n",
        "        predicted_arr = predicted[i, :, :, :].clone().detach().cpu().numpy().argmax(0)\n",
        "        \n",
        "        same = (target_arr == predicted_arr).sum()\n",
        "        a, b = target_arr.shape\n",
        "        total = a*b\n",
        "        accsum += same/total\n",
        "    \n",
        "    pixelAccuracy = accsum/target.shape[0]        \n",
        "    return pixelAccuracy"
      ],
      "metadata": {
        "id": "8m_TZSwiPkyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, lr_scheduler, model, loss_fn, val_loader, train_loader = None, lastCkptPath = None):\n",
        "    if torch.cuda.is_available():  \n",
        "        dev = \"cuda:0\" \n",
        "    else:  \n",
        "        dev = \"cpu\"\n",
        "    device = torch.device(dev)\n",
        "    \n",
        "    tr_loss_arr = []\n",
        "    val_loss_arr = []\n",
        "    meanioutrain = []\n",
        "    pixelacctrain = []\n",
        "    meanioutest = []\n",
        "    pixelacctest = []\n",
        "    f1train = []\n",
        "    f1test = []\n",
        "    precisiontrain = []\n",
        "    precisiontest = []\n",
        "    recalltrain = []\n",
        "    recalltest = []\n",
        "    prevEpoch = 0\n",
        "    \n",
        "    if lastCkptPath != None :\n",
        "        checkpoint = torch.load(lastCkptPath)\n",
        "        prevEpoch = checkpoint['epoch']\n",
        "        try:\n",
        "          checkpoint.eval()\n",
        "        except AttributeError as error:\n",
        "          print(error)\n",
        "        model = SegModel() \n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict']) \n",
        "        for state in optimizer.state.values():\n",
        "          for k, v in state.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "              state[k] = v.to(device).tr_loss_arr =  checkpoint['Training Loss']\n",
        "        val_loss_arr =  checkpoint['Validation Loss']\n",
        "        meanioutrain =  checkpoint['MeanIOU train']\n",
        "        pixelacctrain =  checkpoint['PixelAcc train']\n",
        "        meanioutest =  checkpoint['MeanIOU test']\n",
        "        pixelacctest =  checkpoint['PixelAcc test']\n",
        "        # precisiontrain = checkpoint['Precision Train']\n",
        "        # precisiontest = checkpoint['Precision Test']\n",
        "        # recalltrain = checkpoint['Recall Train']\n",
        "        # recalltest = checkpoint['Recall Test']\n",
        "        # f1train = checkpoint['F1 score Train']\n",
        "        # f1test = checkpoint['F1 score Test'] \n",
        "\n",
        "        print(\"loaded model at epoch\", prevEpoch)\n",
        "        model.to(device)\n",
        "    \n",
        "    for epoch in range(0, n_epochs):\n",
        "        if train_loader != None:\n",
        "          train_loss = 0.0\n",
        "          pixelacc = 0\n",
        "          meaniou = 0\n",
        "          \n",
        "          pbar = tqdm(train_loader, total = len(train_loader))\n",
        "          for X, y in pbar:\n",
        "              torch.cuda.empty_cache()\n",
        "              model.train()\n",
        "              X = X.to(device).float()\n",
        "              y = y.to(device).float()\n",
        "              ypred = model(X)\n",
        "              loss = loss_fn(ypred, y)\n",
        "              \n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              \n",
        "              tr_loss_arr.append(loss.item())\n",
        "              meanioutrain.append(meanIOU(y, ypred))\n",
        "              pixelacctrain.append(pixelAcc(y, ypred))\n",
        "              # precisiontrain.append(precision(y, ypred))\n",
        "              # recalltrain.append(recall(y, ypred))\n",
        "              # f1train.append(f1(y, ypred))\n",
        "              pbar.set_postfix({'Epoch':epoch+1+prevEpoch, \n",
        "                                'Training Loss': np.mean(tr_loss_arr),\n",
        "                                'Mean IOU': np.mean(meanioutrain),\n",
        "                                'Pixel Acc': np.mean(pixelacctrain),\n",
        "                                # 'Precision': np.mean(precisiontrain),\n",
        "                                # 'Recall':np.mean(recalltrain),\n",
        "                                # 'F1 Score':np.mean(f1train)\n",
        "                              })\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            \n",
        "            val_loss = 0\n",
        "            pbar = tqdm(val_loader, total = len(val_loader))\n",
        "            for X, y in pbar:\n",
        "                torch.cuda.empty_cache()\n",
        "                X = X.to(device).float()\n",
        "                y = y.to(device).float()\n",
        "                model.eval()\n",
        "                ypred = model(X)\n",
        "                \n",
        "                val_loss_arr.append(loss_fn(ypred, y).item())\n",
        "                pixelacctest.append(pixelAcc(y, ypred))\n",
        "                meanioutest.append(meanIOU(y, ypred))\n",
        "                # precisiontest.append(precision(y, ypred))\n",
        "                # recalltest.append(recall(y, ypred))\n",
        "                # f1test.append(f1(y, ypred))\n",
        "                \n",
        "                pbar.set_postfix({'Epoch':epoch+1+prevEpoch, \n",
        "                                  'Validation Loss': np.mean(val_loss_arr),\n",
        "                                  'Mean IOU': np.mean(meanioutest),\n",
        "                                  'Pixel Acc': np.mean(pixelacctest),\n",
        "                                  # 'Precision': np.mean(precisiontest),\n",
        "                                  # 'Recall':np.mean(recalltest),\n",
        "                                  # 'F1 Score':np.mean(f1test)\n",
        "                                 })\n",
        "        \n",
        "        \n",
        "        \n",
        "        checkpoint = {\n",
        "            'epoch':epoch+1+prevEpoch,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'Training Loss': tr_loss_arr,\n",
        "            'Validation Loss':val_loss_arr,\n",
        "            'MeanIOU train':meanioutrain, \n",
        "            'PixelAcc train':pixelacctrain, \n",
        "            'MeanIOU test':meanioutest, \n",
        "            'PixelAcc test':pixelacctest,\n",
        "            # 'Precision Train': precisiontrain, \n",
        "            # 'Precision Test': precisiontest, \n",
        "            # 'Recall Train': recalltrain, \n",
        "            # 'Recall Test': recalltest, \n",
        "            # 'F1 score Train': f1train, \n",
        "            # 'F1 score Test': f1test, \n",
        "        }\n",
        "        torch.save(checkpoint, '/content/best_modelDeeplabSGD.pth')\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "    return np.mean(tr_loss_arr),np.mean(val_loss_arr),np.mean(meanioutrain),np.mean(pixelacctrain), np.mean(meanioutest), np.mean(pixelacctest) #, f1train, f1test, precisiontrain, precisiontest, recalltrain ,recalltest"
      ],
      "metadata": {
        "id": "8NqdwivbPnMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get epochs and augmented datasets for training and validation sets"
      ],
      "metadata": {
        "id": "PQ8UNIgQPrwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import albumentations\n",
        "# Get train and val dataset instances\n",
        "train_dataset = SemanticSegmentationDataset(\n",
        "    root_dir='/content/data', \n",
        "    train = 1,\n",
        "    augmentation = get_augmentation(),\n",
        "    preprocessing = get_preprocessing(preprocessing_fn)\n",
        ")\n",
        "\n",
        "valid_dataset = SemanticSegmentationDataset(\n",
        "    root_dir = '/content/data', \n",
        "    train=3,\n",
        "    augmentation = get_validation_augmentation(),\n",
        "    preprocessing = get_preprocessing(preprocessing_fn)\n",
        ")"
      ],
      "metadata": {
        "id": "SV-ckuNMPprK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloaders"
      ],
      "metadata": {
        "id": "VuNPkhC1PvX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get train and val data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=0, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=20, shuffle=False, num_workers=0, pin_memory=True)"
      ],
      "metadata": {
        "id": "wm8wkFzoPu2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the model**\n",
        "When tested in range 1-25, the optimal so far is 3 epochs, but feel free to adjust the number at EPOCHS in line 12"
      ],
      "metadata": {
        "id": "pQL972AFP0kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "retval = training_loop(5, \n",
        "                       optimizer, \n",
        "                       lr_scheduler, \n",
        "                       model, \n",
        "                       loss, \n",
        "                       valid_loader,\n",
        "                       train_loader, \n",
        "                       )"
      ],
      "metadata": {
        "id": "tiUudXEJPzke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load best saved model checkpoint from the current run\n",
        "ckp = \"\"\n",
        "if os.path.exists('/content/data/best_modelDeeplabSGD.pth'):\n",
        "    model = torch.load('/content/data/DeeplabSGD.pth', map_location=DEVICE)\n",
        "    ckp = '/content/data/DeeplabSGD.pth'\n",
        "    print('Loaded DeeplabSGDNet model from this run.')\n",
        "\n",
        "elif os.path.exists('/content/drive/MyDrive/Colab Notebooks/best_modelDeeplabSGD3.pth'):\n",
        "    model = torch.load('/content/drive/MyDrive/Colab Notebooks/best_modelDeeplabSGD3.pth', map_location=DEVICE)\n",
        "    ckp = '/content/drive/MyDrive/Colab Notebooks/best_modelDeeplabSGD3.pth'\n",
        "    print('Loaded DeeplabSGDNet model from previous run.')"
      ],
      "metadata": {
        "id": "bLpaDwiWP5vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run model on **testing set**"
      ],
      "metadata": {
        "id": "UPbdtWixP-NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create test dataloader (with preprocessing operation: to_tensor(...))\n",
        "test_dataset = SemanticSegmentationDataset(\n",
        "    root_dir = \"/content/data\", \n",
        "    train = 2,\n",
        ")\n",
        "\n",
        "# test dataset for visualization (without preprocessing augmentations & transformations)\n",
        "# get a random test image/mask index\n",
        "random_idx = np.random.randint(0, len(test_dataset)-1)\n",
        "image, mask = test_dataset[random_idx]\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
        ")"
      ],
      "metadata": {
        "id": "fEFQ5wCKP7_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_folder = '/content/predictionsDeeplabSGD/'\n",
        "if not os.path.exists(preds_folder):\n",
        "    os.makedirs(\"/content/predictionsDeeplabSGD/\")"
      ],
      "metadata": {
        "id": "s4Lq5K0CQBnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SemanticSegmentationDataset(\n",
        "    root_dir='/content/data', \n",
        "    train = 2,\n",
        "    augmentation=get_validation_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn)\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=True, num_workers=0, pin_memory=True)"
      ],
      "metadata": {
        "id": "V1_2BdqPQDmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference, Metrics Evaluation and Visualisation**"
      ],
      "metadata": {
        "id": "XvIoTUFeQGsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "testmodel = SegModel() \n",
        "testmodel.to(DEVICE)\n",
        "retval = training_loop(1, \n",
        "                       optimizer, \n",
        "                       lr_scheduler, \n",
        "                       model, \n",
        "                       loss, \n",
        "                       test_dataloader,\n",
        "                       lastCkptPath=ckp)\n",
        "\n",
        "testmodel.load_state_dict(model['state_dict'])\n",
        "testmodel.to(DEVICE).float()"
      ],
      "metadata": {
        "id": "veJoYt_nQGFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_loss_arr, val_loss_arr, meanioutrain, pixelacctrain, meanioutest, pixelacctest = retval\n",
        "print(\"Evaluation on Test Data: \")\n",
        "print(f\"Mean Accuracy: {pixelacctest:.4f}\")\n",
        "print(f\"Mean Loss: {val_loss_arr:.4f}\")\n",
        "print(f\"Mean IoU Score: {meanioutest:.4f}\")"
      ],
      "metadata": {
        "id": "2fDdzw1XQL3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## For sample illustrations\n",
        "# Center crop padded image / mask to original image dims\n",
        "def crop_image(image, true_dimensions):\n",
        "    return album.CenterCrop(p=1,height=304, width=304)(image=image)\n",
        "\n",
        "# test dataset for visualization\n",
        "test_dataset_vis = SemanticSegmentationDataset(\n",
        "    root_dir='/content/data',\n",
        "    augmentation = get_validation_augmentation(),\n",
        "    train = 2,\n",
        ")"
      ],
      "metadata": {
        "id": "_CgReCTZQOyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "for idx in range(500):\n",
        "    image, gt_mask = test_dataset[idx]\n",
        "    image_vis = test_dataset_vis[idx][0].astype('uint8')\n",
        "    true_dimensions = image_vis.shape\n",
        "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    # Predict test image\n",
        "    pred_mask = testmodel(x_tensor)\n",
        "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
        "    # Convert pred_mask from `CHW` format to `HWC` format\n",
        "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
        "    pred_mask = cv.resize(pred_mask,(304,304))\n",
        "\n",
        "    # Get prediction channel corresponding to sky, or any classes available\n",
        "    pred_foreground_heatmap = crop_image(pred_mask[:,:,CLASSES.index(\"sky\")], true_dimensions)['image']\n",
        "    pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values), true_dimensions)['image']\n",
        "    # Convert gt_mask from `CHW` format to `HWC` format\n",
        "    gt_mask = np.transpose(gt_mask,(1,2,0))\n",
        "    gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values), true_dimensions)['image']\n",
        "    cv.imwrite(os.path.join(preds_folder, f\"pred_{idx}.png\"), np.hstack([pred_mask])[::])\n",
        "    \n",
        "    visualize(\n",
        "        original_image = image_vis,\n",
        "        ground_truth_mask = gt_mask,\n",
        "        predicted_mask = pred_mask,\n",
        "        pred_foreground_heatmap = pred_foreground_heatmap\n",
        "    )\n",
        "    "
      ],
      "metadata": {
        "id": "cKzxGswdQVdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Copy all info:\n",
        "with open('inferenceDeeplabSGD.txt', 'w') as convert_file:\n",
        "  convert_file.write(\"\\nTest loss\")\n",
        "  convert_file.write(str(val_loss_arr))\n",
        "  convert_file.write(\"\\nMean IOU test\")\n",
        "  convert_file.write(str(meanioutest))\n",
        "  convert_file.write(\"\\nPixel accuracy test\")\n",
        "  convert_file.write(str(pixelacctest))"
      ],
      "metadata": {
        "id": "2ZB14y9IQaAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Download predictions and trained model\n",
        "!zip -r /content/predictionsDeeplabSGD.zip /content/predictionsDeeplabSGD\n",
        "from google.colab import files\n",
        "files.download(\"/content/predictionsDeeplabSGD.zip\")\n",
        "files.download(\"/content/best_modelDeeplabSGD.pth\")\n",
        "files.download(\"/content/data/inferenceDeeplabSGD.txt\")"
      ],
      "metadata": {
        "id": "MKtDhoe2Qaxv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}